{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forbes Articles Crawling\n",
    "\n",
    "For data, articles on various topics (such as:\n",
    "[Money](https://www.forbes.com/money/),\n",
    "[Leadership](https://www.forbes.com/leadership/),\n",
    "[Worlds-Billionaires](https://www.forbes.com/worlds-billionaires/),\n",
    "[Business](https://www.forbes.com/business/),\n",
    "[Small Business](https://www.forbes.com/small-business/),\n",
    "[Life Style](https://www.forbes.com/lifestyle/),\n",
    "[Real State](https://www.forbes.com/real-estate/) and etc\n",
    ")\n",
    "are extracted from the pages of \n",
    "[Forbes](www.forbes.com) website.\n",
    "\n",
    "> [Forbes](https://en.wikipedia.org/wiki/Forbes) is an American business magazine owned by Integrated Whale Media Investments and the Forbes family. Published eight times a year, it features articles on finance, industry, investing, and marketing topics. Forbes also reports on related subjects such as technology, communications, science, politics, and law.\n",
    "\n",
    "**Scrapy** library in python was used to extract articles. The information extracted corresponding to each\n",
    "article is as follows:\n",
    "\n",
    "| Attribute | Description | Type |\n",
    "| -- | -- | -- |\n",
    "|context_header| category (context) of article | String |\n",
    "|corpus_date_ymd| date of article publication (y/m/d) | String | \n",
    "|corpus_date_hm| date of release (h/m) | String |\n",
    "|corpus_title| title of article | String |\n",
    "|corpus_content_paragraphs | paragraphs of article content | List(String) |\n",
    "|author_var_dict | profile of article author (described bellow.) | Dictionary |\n",
    "\n",
    "So that <code>author_var_dict</code> attribute contains the following fields:\n",
    "\n",
    "| Attribute | Description | Type |\n",
    "| -- | -- | -- |\n",
    "|author_forbes_url| forbes url of article author | String | \n",
    "| author_name | article author name | String |\n",
    "| author_contrib_type | article author contributer type | String |\n",
    "| author_subcontext_header | author field | String |\n",
    "| author_about | a paragraph about article author | String |\n",
    "| author_social_links | article author social links | List(String) | \n",
    "\n",
    "**Scrapy** library is installed with the following command:\n",
    "\n",
    "<code>pip3 install scrapy</code>\n",
    "\n",
    "and the current articles on the Forbes site are extracted in <code>file_name.json</code> using the following command:\n",
    "\n",
    "<code>scrapy crawl forbes -O file_name.json</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Json Files Data \n",
    "\n",
    "In this section, <code>f{i}.json</code> format json files are aggregated and repeated items are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 2\n",
    "data = []\n",
    "for i in range(n):\n",
    "    file_i = open(f'f{i + 1}.json')\n",
    "    dataset_i = json.load(file_i)\n",
    "    data.extend(dataset_i)\n",
    "\n",
    "titles = []\n",
    "data_unique = []\n",
    "for item in data:\n",
    "    title = item['corpus_title']\n",
    "    if title not in titles and 'author_var_dict' in item.keys():\n",
    "        titles.append(title)\n",
    "        data_unique.append(item)\n",
    "\n",
    "data = data_unique\n",
    "\n",
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dictionary to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.json_normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"corpus_content_parts\"] = df[\"corpus_content_parts\"].apply(lambda x: list(filter(lambda item: item not in [\"Share to Facebook\", \"Share to Twitter\", \"Share to Linkedin\"] ,x)))\n",
    "df[\"corpus_content\"] = df['corpus_content_parts'].apply(lambda x: \" \".join(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Samples From Contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>corpus_content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>context_header</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Billionaires</th>\n",
       "      <th>133</th>\n",
       "      <td>Japanese auto maker Suzuki Motor plans to inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Business</th>\n",
       "      <th>53</th>\n",
       "      <td>With Paramount’s  The Lost City  earning decen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Leadership</th>\n",
       "      <th>137</th>\n",
       "      <td>I nflation may be hitting  new 40-year highs ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lifestyle</th>\n",
       "      <th>26</th>\n",
       "      <td>For hip hop star and fashion icon A$AP Rocky, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Money</th>\n",
       "      <th>91</th>\n",
       "      <td>Key News Asian equities were mixed overnight a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Real Estate</th>\n",
       "      <th>102</th>\n",
       "      <td>The Biden administration has established a tas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Small Business</th>\n",
       "      <th>48</th>\n",
       "      <td>Most companies want to grow. Often, the very s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       corpus_content\n",
       "context_header                                                       \n",
       "Billionaires   133  Japanese auto maker Suzuki Motor plans to inve...\n",
       "Business       53   With Paramount’s  The Lost City  earning decen...\n",
       "Leadership     137  I nflation may be hitting  new 40-year highs ,...\n",
       "Lifestyle      26   For hip hop star and fashion icon A$AP Rocky, ...\n",
       "Money          91   Key News Asian equities were mixed overnight a...\n",
       "Real Estate    102  The Biden administration has established a tas...\n",
       "Small Business 48   Most companies want to grow. Often, the very s..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = df[\"corpus_content\"].apply(lambda x: x.replace(\"\", \"\"))\n",
    "samples = df.groupby(\"context_header\").apply(lambda x: x.sample(n=1))[\"corpus_content\"]\n",
    "pd.DataFrame(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download And Setup Pre-Trained Model\n",
    "\n",
    "To download and setup spacy pre-trained language model, uncomment the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Specifications of Corpus (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_basic_specefications_of(text):\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # collect doc.sents generator \n",
    "    sentences = list(doc.sents)\n",
    "\n",
    "    # doc tokens\n",
    "    tokens = doc[:]\n",
    "\n",
    "    n_all_tokens = len(tokens)\n",
    "    n_sentences = len(sentences)\n",
    "\n",
    "    average_sentence_len = sum(len(sentence) for sentence in sentences) / n_sentences\n",
    "\n",
    "    n_puncs = sum(map(lambda token: token.is_punct, tokens))\n",
    "    n_stopwords = sum(map(lambda token: token.is_stop, tokens))\n",
    "\n",
    "    # print(f'{n_sentences: <5}{n_total_tokens: <5}{n_puncs: <5}{n_stopwords: <5}')\n",
    "    return n_sentences, average_sentence_len, n_all_tokens, n_puncs, n_stopwords\n",
    "\n",
    "\n",
    "df[['No Sentences', 'Average Lengths Of Sentences', 'No All Tokens', 'No Punctuation Tokens', 'No Stopwords']] = df.apply(lambda item: extract_basic_specefications_of(item['corpus_content']), axis=1,\n",
    "                                                                                                                          result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For normalization, the following functions are applied to the text of the given articles, respectively:\n",
    "<center>\n",
    "    $\\text{Input Text} \\Rightarrow \\text{Lemmatize} \\Rightarrow \\text{Remove Stopwords} \\Rightarrow \\text{Remove Punctuations} \\Rightarrow \\text{Output Tokens}$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalizeـcomponent(doc: str):\n",
    "    doc = doc.lower().strip()\n",
    "\n",
    "    doc = nlp(doc)\n",
    "\n",
    "    # lemmatization\n",
    "    r = map(lambda token: token.lemma_, doc)\n",
    "\n",
    "    # remove whitespaces\n",
    "    r = map(lambda token: token.strip(), r)\n",
    "\n",
    "    # remove stopwords    \n",
    "    r = list(filter(lambda token: not nlp.vocab[token].is_stop, r))\n",
    "\n",
    "    # remove punctuations\n",
    "    r = list(filter(lambda token: token not in string.punctuation, r))\n",
    "\n",
    "    return r\n",
    "\n",
    "\n",
    "df['normalized_corpus_words'] = df['corpus_content'].map(normalizeـcomponent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Specification of Corpus (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_other_specefications_of(words):\n",
    "    n_unique_words = len(set(words))\n",
    "\n",
    "    n_words = len(words)\n",
    "    average_word_len = sum(len(word) for word in words) / n_words\n",
    "    longest_word_len = max(len(word) for word in words)\n",
    "\n",
    "    return n_words, n_unique_words, average_word_len, longest_word_len\n",
    "\n",
    "\n",
    "df[['No Words', 'No Unique Words', 'Average Words Length', 'Longest Word Length']] = df.apply(lambda item: extract_other_specefications_of(item['normalized_corpus_words']), axis=1,\n",
    "                                                                                              result_type='expand')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Most Frequent Words of Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "context_header\n",
       "Billionaires      [private, jet, priceless, diamond, run, law, —...\n",
       "Business          [record, £, 5.6, billion, 7.3, billion, spend,...\n",
       "Leadership        [medium, technology, tool, fuel, information, ...\n",
       "Lifestyle         [entertain, easter, come, april, 17, lot, pres...\n",
       "Money             [real, earning, season, february, 14, –, march...\n",
       "Real Estate       [cliffside, malibu, estate, belong, supermodel...\n",
       "Small Business    [ceo, co, founder, inspirant, group, curator, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_n_most_frequent_word(words: list, n=20):\n",
    "    word_freq = Counter(words)\n",
    "    common_words_and_frequency = word_freq.most_common(n)\n",
    "\n",
    "    return common_words_and_frequency\n",
    "\n",
    "\n",
    "df['Common Words and Frequencies'] = df['normalized_corpus_words'].map(extract_n_most_frequent_word)\n",
    "\n",
    "gb = df.groupby(\"context_header\").apply(lambda x: list(chain.from_iterable(x['normalized_corpus_words'].tolist())))\n",
    "gb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate Statistics About Articls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context_header</th>\n",
       "      <th>corpus_title</th>\n",
       "      <th>No Sentences</th>\n",
       "      <th>Average Lengths Of Sentences</th>\n",
       "      <th>No All Tokens</th>\n",
       "      <th>No Punctuation Tokens</th>\n",
       "      <th>No Stopwords</th>\n",
       "      <th>No Words</th>\n",
       "      <th>No Unique Words</th>\n",
       "      <th>Average Words Length</th>\n",
       "      <th>Longest Word Length</th>\n",
       "      <th>Common Words and Frequencies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>Small Business</td>\n",
       "      <td>Trebel Hopes Going Public Will Take It To The ...</td>\n",
       "      <td>37.0</td>\n",
       "      <td>24.432432</td>\n",
       "      <td>904.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>392.0</td>\n",
       "      <td>397.0</td>\n",
       "      <td>241.0</td>\n",
       "      <td>5.846348</td>\n",
       "      <td>13.0</td>\n",
       "      <td>[(music, 15), (mekikian, 13), (trebel, 11), (–...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Leadership</td>\n",
       "      <td>Lottery Numbers, Blockchain Articles And Cold ...</td>\n",
       "      <td>62.0</td>\n",
       "      <td>23.967742</td>\n",
       "      <td>1486.0</td>\n",
       "      <td>188.0</td>\n",
       "      <td>598.0</td>\n",
       "      <td>663.0</td>\n",
       "      <td>407.0</td>\n",
       "      <td>5.974359</td>\n",
       "      <td>15.0</td>\n",
       "      <td>[(russian, 14), (news, 10), (russia, 10), (rsf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>Billionaires</td>\n",
       "      <td>Mutual Fund Billionaire Edward “Ned” Johnson I...</td>\n",
       "      <td>30.0</td>\n",
       "      <td>22.833333</td>\n",
       "      <td>685.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>343.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>5.822157</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[(johnson, 20), (fidelity, 10), (company, 9), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Money</td>\n",
       "      <td>Is Charles Schwab Stock Fairly Priced?</td>\n",
       "      <td>23.0</td>\n",
       "      <td>26.739130</td>\n",
       "      <td>615.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>302.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>5.390728</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[(revenue, 13), (net, 11), (billion, 9), (y, 7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Business</td>\n",
       "      <td>Aspirin Improves Survival For Hospitalized Cov...</td>\n",
       "      <td>13.0</td>\n",
       "      <td>38.076923</td>\n",
       "      <td>495.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>183.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>6.004000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>[(aspirin, 10), (patient, 10), (hospital, 7), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>Faith Connexion Sets The Tone For AW22 With Se...</td>\n",
       "      <td>18.0</td>\n",
       "      <td>33.944444</td>\n",
       "      <td>611.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>217.0</td>\n",
       "      <td>6.027119</td>\n",
       "      <td>22.0</td>\n",
       "      <td>[(faith, 11), (new, 9), (collection, 7), (conn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>Billionaires</td>\n",
       "      <td>Malaysian Billionaire Stanley Thai Invests $35...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>30.333333</td>\n",
       "      <td>364.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>6.105882</td>\n",
       "      <td>13.0</td>\n",
       "      <td>[(supermax, 9), (u.s, 7), (glove, 5), (year, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>Lifestyle</td>\n",
       "      <td>Boeing 737 Crash: Why It Could Be Years Before...</td>\n",
       "      <td>31.0</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>806.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>341.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>6.137830</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[(recorder, 6), (baier, 6), (aviation, 6), (ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>Leadership</td>\n",
       "      <td>4 Reasons Why SMS Is The Future Of E-Commerce</td>\n",
       "      <td>81.0</td>\n",
       "      <td>15.888889</td>\n",
       "      <td>1287.0</td>\n",
       "      <td>187.0</td>\n",
       "      <td>514.0</td>\n",
       "      <td>567.0</td>\n",
       "      <td>312.0</td>\n",
       "      <td>5.798942</td>\n",
       "      <td>14.0</td>\n",
       "      <td>[(marketing, 20), (sms, 14), (sm, 12), (email,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>Billionaires</td>\n",
       "      <td>PUBG Developer Teams Up With Solana Labs To La...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>34.250000</td>\n",
       "      <td>411.0</td>\n",
       "      <td>48.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>149.0</td>\n",
       "      <td>6.228155</td>\n",
       "      <td>15.0</td>\n",
       "      <td>[(blockchain, 10), (game, 8), (billionaire, 5)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     context_header                                       corpus_title  \\\n",
       "121  Small Business  Trebel Hopes Going Public Will Take It To The ...   \n",
       "99       Leadership  Lottery Numbers, Blockchain Articles And Cold ...   \n",
       "130    Billionaires  Mutual Fund Billionaire Edward “Ned” Johnson I...   \n",
       "96            Money             Is Charles Schwab Stock Fairly Priced?   \n",
       "60         Business  Aspirin Improves Survival For Hospitalized Cov...   \n",
       "105       Lifestyle  Faith Connexion Sets The Tone For AW22 With Se...   \n",
       "72     Billionaires  Malaysian Billionaire Stanley Thai Invests $35...   \n",
       "39        Lifestyle  Boeing 737 Crash: Why It Could Be Years Before...   \n",
       "138      Leadership      4 Reasons Why SMS Is The Future Of E-Commerce   \n",
       "80     Billionaires  PUBG Developer Teams Up With Solana Labs To La...   \n",
       "\n",
       "     No Sentences  Average Lengths Of Sentences  No All Tokens  \\\n",
       "121          37.0                     24.432432          904.0   \n",
       "99           62.0                     23.967742         1486.0   \n",
       "130          30.0                     22.833333          685.0   \n",
       "96           23.0                     26.739130          615.0   \n",
       "60           13.0                     38.076923          495.0   \n",
       "105          18.0                     33.944444          611.0   \n",
       "72           12.0                     30.333333          364.0   \n",
       "39           31.0                     26.000000          806.0   \n",
       "138          81.0                     15.888889         1287.0   \n",
       "80           12.0                     34.250000          411.0   \n",
       "\n",
       "     No Punctuation Tokens  No Stopwords  No Words  No Unique Words  \\\n",
       "121                  120.0         392.0     397.0            241.0   \n",
       "99                   188.0         598.0     663.0            407.0   \n",
       "130                   78.0         252.0     343.0            225.0   \n",
       "96                    87.0         206.0     302.0            165.0   \n",
       "60                    56.0         183.0     250.0            158.0   \n",
       "105                   79.0         219.0     295.0            217.0   \n",
       "72                    34.0         139.0     170.0            125.0   \n",
       "39                   107.0         341.0     341.0            229.0   \n",
       "138                  187.0         514.0     567.0            312.0   \n",
       "80                    48.0         136.0     206.0            149.0   \n",
       "\n",
       "     Average Words Length  Longest Word Length  \\\n",
       "121              5.846348                 13.0   \n",
       "99               5.974359                 15.0   \n",
       "130              5.822157                 14.0   \n",
       "96               5.390728                 14.0   \n",
       "60               6.004000                 13.0   \n",
       "105              6.027119                 22.0   \n",
       "72               6.105882                 13.0   \n",
       "39               6.137830                 14.0   \n",
       "138              5.798942                 14.0   \n",
       "80               6.228155                 15.0   \n",
       "\n",
       "                          Common Words and Frequencies  \n",
       "121  [(music, 15), (mekikian, 13), (trebel, 11), (–...  \n",
       "99   [(russian, 14), (news, 10), (russia, 10), (rsf...  \n",
       "130  [(johnson, 20), (fidelity, 10), (company, 9), ...  \n",
       "96   [(revenue, 13), (net, 11), (billion, 9), (y, 7...  \n",
       "60   [(aspirin, 10), (patient, 10), (hospital, 7), ...  \n",
       "105  [(faith, 11), (new, 9), (collection, 7), (conn...  \n",
       "72   [(supermax, 9), (u.s, 7), (glove, 5), (year, 5...  \n",
       "39   [(recorder, 6), (baier, 6), (aviation, 6), (ch...  \n",
       "138  [(marketing, 20), (sms, 14), (sm, 12), (email,...  \n",
       "80   [(blockchain, 10), (game, 8), (billionaire, 5)...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 10\n",
    "\n",
    "df[['context_header',\n",
    "    'corpus_title',\n",
    "    'No Sentences',\n",
    "    'Average Lengths Of Sentences',\n",
    "    'No All Tokens',\n",
    "    'No Punctuation Tokens',\n",
    "    'No Stopwords',\n",
    "    'No Words',\n",
    "    'No Unique Words',\n",
    "    'Average Words Length',\n",
    "    'Longest Word Length',\n",
    "    'Common Words and Frequencies'\n",
    "    ]].sample(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Keywords Of Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['earning', 'gaap', 'gaap earning', 'million', 'analyst']\n",
      "['fink', 'russia', 'war', 'globalization', 'world order']\n",
      "['russian', 'sanction', 'putin', 'russia', 'treasury']\n",
      "['dao', 'hoenisch', 'ethereum', 'hacker', 'attacker']\n",
      "['loan', 'student loan', 'payment', 'refund', 'student']\n",
      "['ftx', 'license', 'binance', 'crypto', '— —']\n",
      "['tax', 'retirement', 'bill', 'r&d', 'restore']\n",
      "['climate', 'sec', 'disclosure', 'information', 'investor']\n",
      "['loan', 'student loan', 'student', 'borrower', 'payment']\n",
      "['dei', 'bellan', 'bellan white', 'tech', 'journalism']\n",
      "['leviev', 'diamond', 'de beer', 'beer', 'de']\n",
      "['skill', 'shortage', 'industry', 'film', 'coach']\n",
      "['team', 'lesson', 'culture', 'behavior', 'eternal']\n",
      "['ham', 'biscuit', 'lamb', 'meat', 'mail order']\n",
      "['malibu', 'beach', 'mansion', 'floor', 'crawford']\n",
      "['lease', 'sentral', 'apartment', 'landing', 'residence']\n",
      "['symbium', 'city', 'housing', 'adu', 'planning department']\n",
      "['bathroom', 'renovation', 'diy', 'tile', 'budget']\n",
      "['eqt', 'bpea', 'private equity', 'private', 'firm']\n",
      "['housing', 'home', 'covid', 'accessible', 'building']\n",
      "['finger', 'houston', 'apartment', 'build', 'unit']\n",
      "['residence', 'central park', 'central', 'tower', 'central park tower']\n",
      "['garden', 'farrand', 'landscape', 'woman', 'landscape architect']\n",
      "['moon', 'henderson', 'lunar', 'surface', 'lunar surface']\n",
      "['abbas', 'yeezy', 'west', 'ghize', 'yeezy brand']\n",
      "['·', 'bar', 'mckenna', 'drink', 'oz']\n",
      "['whisky', 'mercer prince', 'mercer', 'rocky', 'prince']\n",
      "['cresco', 'columbia', 'columbia care', 'care', 'cannabis']\n",
      "['auto', 'auto sale', 'j.d', 'j.d power', 'march']\n",
      "['social equity', 'boston', 'equity', 'massachusetts', 'c3']\n",
      "['lauren', 'kohl', 'collection', 'sonoma', 'lane']\n",
      "['cleveland', 'beer', 'brewery', 'brew', 'city']\n",
      "['door', '2 door', 'bronco', 'ford', '2']\n",
      "['cocina', 'la cocina', 'la', 'cuisine', 'municipal marketplace']\n",
      "['circle line', 'circle', 'cruise', 'line', 'hudson']\n",
      "['cristal', 'roederer', 'vintage', 'wine', 'champagne']\n",
      "['ailey', 'ailey ii', 'harper', 'ii', 'dance']\n",
      "['woman', 'seneca', 'seneca fall', 'declaration', 'rights']\n",
      "['tariff', 'uk', 'whiskey', 'alcohol', 'spirit']\n",
      "['aviation', 'baier', 'recorder', 'crash', 'accident']\n",
      "['scent', 'cologne', 'fragrance', 'shop online', 'note']\n",
      "['wine', 'winery', 'drink', 'purple rain', 'rain']\n",
      "['salon', 'haircare', 'client', 'product', 'collective']\n",
      "['aurora', 'cannabis', 'aurora cannabis', 'terrafarma', 'thrive']\n",
      "['pistol', 'tourist', 'square', 'gritti', 'gull']\n",
      "['li', 'wana', 'cannabis', 'wana brand', 'cannabis industry']\n",
      "['oz', 'garnish', 'fairmont', 'ingredient', 'gin']\n",
      "['sorokin', 'martine', 'art', 'anna', 'tell']\n",
      "['vajre', 'expand', 'coverage', 'expansion', 'expand coverage']\n",
      "['customer', 'repeat', 'repeat customer', 'customer service', 'brand advocate']\n",
      "['quantum', 'aq', 'sandbox', 'sandbox aq', 'schmidt']\n",
      "['tech', 'woman', 'gender', 'female', 'diversity']\n",
      "['habit', 'bad habit', 'bad', 'trigger', 'cue']\n",
      "['bullock', 'million', 'million adjust', 'movie', 'star']\n",
      "['woman', 'feeke', 'untold', 'biography', 'goodread']\n",
      "['jackson', 'aba', 'court', 'child pornography', 'pornography']\n",
      "['spotify', 'artist', 'stream', 'pay', 'streaming']\n",
      "['food', 'fertilizer', 'agriculture', 'price', 'wheat']\n",
      "['bridgerton', 'coughlan', 'lady whistledown', 'whistledown', 'lady']\n",
      "['4.0', 'industry 4.0', 'industrial', 'technology', 'germany']\n",
      "['aspirin', 'patient', 'hospital', 'blood', 'drug']\n",
      "['manufacturer', 'datum', 'ball bearing', 'machine', 'bearing']\n",
      "['gamefam', 'roblox', 'game', 'gaming', 'developer']\n",
      "['card', 'charizard', 'pokémon', 'pokémon card', 'grade']\n",
      "['stubhub', 'mlb', 'ticket', 'sox', 'ticket sale']\n",
      "['space', 'commercial', 'commercial space', 'space company', 'gen']\n",
      "['azarov', 'lush', 'store', 'russia', 'ukraine']\n",
      "['walgreen', 'boot', 'asda', 'capital', 'u.k']\n",
      "['job', 'company culture', 'culture', 'retention strategy', 'strong company']\n",
      "['yacht', 'detain', 'boat', 'authority', 'superyacht']\n",
      "['ai', 'democratization', 'drive car', 'car', 'self drive']\n",
      "['martin', 'coach', 'schmidt', 'school', 'bonaventure']\n",
      "['supermax', 'glove', 'nitrile', 'nitrile glove', 'u.s']\n",
      "['opensea', 'nft', 'finzer', 'atallah', 'million']\n",
      "['yacht', 'russian', 'axioma', 'gibraltar', 'pumpyansky']\n",
      "['scott', 'donation', '2022 include', 'editor note', 'million']\n",
      "['fosun', 'guo', 'lanvin', 'billion yuan', 'yuan']\n",
      "['geely', 'auto', 'li', 'china', 'chip']\n",
      "['checkout.com', 'pousaz', 'billion', 'estimate', 'payment']\n",
      "['tencent', 'lau', 'billion', 'game', 'growth']\n",
      "['blockchain', 'solana', 'game', 'solana lab', 'crypto']\n",
      "['vela', 'akin gump', 'gump', 'akin', 'texas']\n",
      "['russia', 'statement', 'firm', 'corporate', 'business']\n",
      "['work', 'employee', 'workplace', 'compensation', 'lack']\n",
      "['disability', 'case manager', 'skill', 'job', 'manager']\n",
      "['disability', 'people disability', 'accessibility', 'people', 'technology']\n",
      "['rihanna', 'branding', 'makeup', 'fenty', 'brand']\n",
      "['workplace', 'culture', 'fairness', '•', 'work']\n",
      "['tass', 'image', 'russian', 'photo', 'getty']\n",
      "['nomination', 'company', 'fintech', 'business', 'list']\n",
      "['student loan', 'loan', 'student', 'borrower', 'loan borrower']\n",
      "['yesterday', 'pcaob', 'chinese', 'chinese company', 'bond']\n",
      "['scope 3', 'scope', 'company', 'sec', 'esg']\n",
      "['qqq', 'qqqx', 'dividend', 'fund', 'keith']\n",
      "['yen', 'kuroda', 'japan', 'abe', 'tokyo']\n",
      "['vaccine', 'patient', 'healthcare', 'telehealth', 'mrna']\n",
      "['revenue', 'net', 'y', 'charles schwab', 'schwab']\n",
      "['rent', 'housing', 'price', 'inflation', 'income']\n",
      "['taxis', 'lifetime', 'benefit', 'social security', 'medicare']\n",
      "['russian', 'rsf', 'censor', 'russians', 'news']\n",
      "['abramovich', 'peace', 'sanction', 'peace talk', 'zelensky']\n",
      "['sleep', 'bedroom', 'child', 'schneeberg', 'teen']\n",
      "['appraisal', 'racial', 'bias', 'pave', 'task force']\n",
      "['cruz', 'book', 'jackson', 'baby', 'ted']\n",
      "['love', 'theater', 'favorite', 'amazing', 'theatre']\n",
      "['faith', 'connexion', 'faith connexion', 'collection', 'new collection']\n",
      "['joker', 'batman', 'villain', 'character', 'burn']\n",
      "['maluma', 'royalty maluma', 'royalty', 'macy', 'collection']\n",
      "['club floor', 'club', 'room', 'floor', 'minute']\n",
      "['cocktail', 'calorie', 'sugar', '12', 'juice']\n",
      "['dealership', 'dealer', 'auto', 'yajnik', 'online']\n",
      "['cannabis', '2030', 'frontier datum', 'new frontier', 'new frontier datum']\n",
      "['harwood', 'beauty', 'talent', 'trans', 'prize']\n",
      "['supplement', 'sex', 'shop online', 'shop', 'sexual']\n",
      "['whisky', 'world good', 'distillery', 'good', 'world']\n",
      "['pier', 'navy pier', 'navy', 'august', 'chicago']\n",
      "['process', 'cost', 'task', 'cut cost', 'redesign']\n",
      "['business', 'network', 'partnership', 'market', 'startup']\n",
      "['impact', 'disruption', 'virus', 'report', 'moderate']\n",
      "['raise', 'paley', 'billion', 'late stage', 'goldberg']\n",
      "['bootstrappe', 'business', 'time', 'bootstrappe business', 'waldoch']\n",
      "['mekikian', 'trebel', 'music', '–', 'download']\n",
      "['decision', 'important decision', 'core', 'business', 'decision year']\n",
      "['metaverse', 'small business', 'digital', 'mazza', 'product']\n",
      "['wordle', 'friday', 'word', 'guess', 'o']\n",
      "['wwe', 'mjf', 'aew', 'star', '2024']\n",
      "['hugh', 'sunja', 'korean', 'story', 'episode']\n",
      "['cnn', 'cnn+', 'est', 'news', 'anchor']\n",
      "['dna', 'synthesis', 'dna synthesis', 'enzymatic', 'molecular']\n",
      "['financial year', 'fully guarantee', 'guarantee', 'million fully', 'million fully guarantee']\n",
      "['johnson', 'fidelity', 'edward', 'abigail', 'fidelity investment']\n",
      "['china', 'bernstein', 'spac', 'southeast asia', 'singapore']\n",
      "['monde', 'monde nissin', 'nissin', 'billion peso', 'peso']\n",
      "['suzuki', 'mahindra', 'india', 'ev', 'mahindra mahindra']\n",
      "['innovation', 'competency', 'capability', 'create', 'entrepreneurship']\n",
      "['massmutual', 'employee', '–', 'retain', 'digital']\n",
      "['target group', 'fear', '•', 'marketing', 'objective']\n",
      "['pay', 'salary', 'bersin', 'wage', 'employer']\n",
      "['sms', 'sm', 'marketing', 'email', 'audience']\n",
      "['bretton', 'monetary', 'bretton wood', 'bretton woods', 'monetary system']\n"
     ]
    }
   ],
   "source": [
    "# from rake_nltk import Rake\n",
    "#\n",
    "# r = Rake()\n",
    "#\n",
    "# r.extract_keywords_from_text(df['corpus_content'][0])\n",
    "#\n",
    "# r.get_ranked_phrases_with_scores()\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=lambda x:x, lowercase=False, ngram_range=(1,3))\n",
    "r = tfidf.fit_transform(df['normalized_corpus_words'].tolist())\n",
    "response = r.toarray()\n",
    "feature_array = np.array(tfidf.get_feature_names())\n",
    "\n",
    "tfidf_df = pd.DataFrame(response, columns=feature_array)\n",
    "df['Keywords'] = tfidf_df.apply(lambda row: (row.nlargest(5).index.tolist()), axis='columns')\n",
    "for keywords in df['Keywords'].tolist():\n",
    "    print(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
