{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0da559cc",
   "metadata": {},
   "source": [
    "<img src='../../images/besm.png' width='150px'>\n",
    "<style>\n",
    "    \n",
    "@font-face {font-family: \"B Lotus\"; src: url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.eot\"); src: url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.eot?#iefix\") format(\"embedded-opentype\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.woff2\") format(\"woff2\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.woff\") format(\"woff\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.ttf\") format(\"truetype\"), url(\"//db.onlinewebfonts.com/t/1605a655ba0a3246ce5eca3eaff6c5c2.svg#B Lotus\") format(\"svg\"); }\n",
    "\n",
    "\n",
    "</style>\n",
    "\n",
    "<center style ='font-family: \"B Lotus\";'>بازیابی پیشرفته اطلاعات - درس پردازش زبان‌های طبیعی </center>\n",
    "<center style ='font-family: \"B Lotus\";'> آزمایشگاه پردازش هوشمند متن و زبان و علوم انسانی محاسباتی </center>\n",
    "<br>\n",
    "<center> http://language.ml </center>\n",
    "<center> contact: ehsan.asgari [AT] sharif [dot] edu </center>\n",
    "<center> semesters: Fall 2021 / Spring 2022 (Updated) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39afbfbd",
   "metadata": {},
   "source": [
    "</h1> <h1 style='direction:rtl'>انگلیسی - پیش پردازش و بررسی متن \n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813ebde3",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § نمونه‌ای از متن  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79245418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import tqdm\n",
    "import nltk, re\n",
    "from nltk.corpus import brown\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "#nltk.download('brown')\n",
    "all_categories = brown.categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3079af03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['adventure',\n",
       " 'belles_lettres',\n",
       " 'editorial',\n",
       " 'fiction',\n",
       " 'government',\n",
       " 'hobbies',\n",
       " 'humor',\n",
       " 'learned',\n",
       " 'lore',\n",
       " 'mystery',\n",
       " 'news',\n",
       " 'religion',\n",
       " 'reviews',\n",
       " 'romance',\n",
       " 'science_fiction']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "647e2a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_brown_sentences_religion = list(brown.sents(categories=['religion']))\n",
    "all_brown_sentences_news = list(brown.sents(categories=['news']))\n",
    "\n",
    "all_brown_tagged_sentences = list(brown.tagged_sents(categories=all_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1407e3ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 ) .\n",
      "what next ? ?\n",
      "we cover each of these with our blanket of peas and then at last we have octillion peas corresponding in number to the atoms in the body .\n",
      "To the Zen monk the universe is still populated with `` spiritual beings '' who have to be appeased .\n",
      "1 St. John 12 .\n",
      "So we are the more prepared to give Parker the credit for having taken the right side in an unnecessary controversy , to salute his courage , and to pass on , happily forgetting both him and the entire episode .\n",
      "20 ) .\n",
      "The new birth is necessary because the natural man is spiritually dead and blind .\n",
      "According to this doctrine , the universe was ruled by Heaven , T'ien -- as a natural force , or in the personification of a Supreme Sky-god -- governing all things by means of a process called the Tao , which can be roughly interpreted as `` the Order of the Universe '' or `` the Universal Way '' .\n",
      "But , you see , those who are not mentioned will not resent it .\n",
      "After this form of Indian Buddhism had been introduced into China , it underwent extensive changes .\n",
      "The Catholic priest , though somewhat superficially drawn , easily came out the best .\n",
      "To have Christ dwelling through faith in your hearts .\n",
      "During this period 7,484,268 members were received , yet the net membership now is only 9,910,741 .\n",
      "What we can attempt with some hope of dependable conclusions is to point out the manner in which Christianity entered into particular aspects of the life of the nation .\n",
      "Maybe we are talking about them too much .\n",
      "He says : `` That would reduce neurotic ailments tremendously .\n",
      "Glenn Kittler has been twice to Africa , once spending a week with Dr. Albert Schweitzer .\n",
      "`` You will lose the paper .\n",
      "There is nothing in the whole range of human experience more widely known and universally felt than spirit .\n",
      "As such , one cannot fully understand the thought of the pre-Han and Han periods without knowing the meanings inherent in the Lo Shu ; ;\n",
      "Thus the middle number , 5 , represented Sung-Shan in Honan , Central China ; ;\n",
      "I place His precepts and His leadings above every seeming probability , dismissing cherished convictions and holding the wisdom of man as folly when opposed to Him .\n",
      "3 )\n",
      "Neglect means spiritual paralysis or death .\n",
      "Even Professor Arnold Toynbee , agreeing with his son , does so in these terms : `` Compared to continuing to incur a constant risk of the destruction of the human race , all other evils are lesser evils .\n",
      "they simply could no longer be purchased for missionary purposes .\n",
      "If then any man is in Christ , he is a new creature ( literally , `` He is a new creation ) , the former things have passed away ; ;\n",
      "In a certain sense , interfaith communication parallels diplomatic communication among the nation-states .\n",
      "This whole tendency had an unfortunate effect on Chinese thinking .\n"
     ]
    }
   ],
   "source": [
    "for x in random.sample(all_brown_sentences_religion, 30):\n",
    "    print(' '.join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb4a842",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § نرمالایز کردن  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "723bc839",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_sentence(tokenized_sents, minimum_length=2, stopword_removal=True, stopwords_domain=[], lower_case=False, punctuation_removal=True):\n",
    "    '''\n",
    "    normalization function\n",
    "    '''\n",
    "    normalized_sents = tokenized_sents\n",
    "    \n",
    "    if stopword_removal:\n",
    "        # Remove stopwords in English and also the given domain stopwords\n",
    "        stopwords = [x.lower() for x in nltk.corpus.stopwords.words('english')]\n",
    "        normalized_sents=[[word for word in sentence if (word.lower() not in stopwords_domain + stopwords)] for sentence in tokenized_sents ]\n",
    "\n",
    "    if punctuation_removal:\n",
    "        # Remove punctuations\n",
    "        normalized_sents=[[word for word in sentence if word not in string.punctuation] for sentence in normalized_sents ]\n",
    "\n",
    "    if lower_case:\n",
    "        # Convert everything to lowercase and filter based on a min length\n",
    "        normalized_sents=[[word.lower() for word in sentence if len(word)>minimum_length] for sentence in normalized_sents ]\n",
    "\n",
    "    elif minimum_length>1:\n",
    "        normalized_sents= [[word for word in sentence if len(word)>minimum_length] for sentence in normalized_sents ]        \n",
    "        \n",
    "    return normalized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bca6e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_sentences_religion = normalize_sentence(all_brown_sentences_religion)\n",
    "normalized_sentences_news = normalize_sentence(all_brown_sentences_news)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a63d97e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'investigation', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', 'evidence', 'irregularities', 'took', 'place']\n",
      "\n",
      "['jury', 'said', 'term-end', 'presentments', 'City', 'Executive', 'Committee', 'over-all', 'charge', 'election', 'deserves', 'praise', 'thanks', 'City', 'Atlanta', 'manner', 'election', 'conducted']\n",
      "\n",
      "['September-October', 'term', 'jury', 'charged', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'investigate', 'reports', 'possible', 'irregularities', 'hard-fought', 'primary', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.']\n",
      "\n",
      "['result', 'although', 'still', 'make', 'use', 'distinction', 'much', 'confusion', 'meaning', 'basic', 'terms', 'employed']\n",
      "\n",
      "['meant', 'spirit', 'matter']\n",
      "\n",
      "['terms', 'generally', 'taken', 'granted', 'though', 'referred', 'direct', 'axiomatic', 'elements', 'common', 'experience']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for x in normalized_sentences_news[0:3] + normalized_sentences_religion[0:3]:\n",
    "    print(x)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42518dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word             Frequency        % of the total  \n",
      "God              131              7.634032634032634\n",
      "world            90               5.244755244755245\n",
      "one              87               5.06993006993007\n",
      "may              78               4.545454545454546\n",
      "new              77               4.487179487179487\n",
      "would            68               3.9627039627039626\n",
      "man              64               3.7296037296037294\n",
      "could            59               3.4382284382284385\n",
      "Christ           59               3.4382284382284385\n",
      "also             56               3.263403263403263\n",
      "life             55               3.205128205128205\n",
      "must             54               3.146853146853147\n",
      "church           51               2.972027972027972\n",
      "Christian        50               2.9137529137529135\n",
      "power            49               2.8554778554778557\n",
      "members          49               2.8554778554778557\n",
      "spirit           46               2.6806526806526807\n",
      "many             46               2.6806526806526807\n",
      "human            43               2.505827505827506\n",
      "Church           43               2.505827505827506\n"
     ]
    }
   ],
   "source": [
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "mp_freqdist = FreqDist(itertools.chain(*normalized_sentences_religion))               # compute the frequency distribution\n",
    "top20words=mp_freqdist.most_common(20)      # show the top 20 (word, frequency) pairs\n",
    "print ('%-16s' % 'word', '%-16s' % 'Frequency','%-16s' %  '% of the total')\n",
    "for topword in top20words:\n",
    "    percent=(topword[1]/len(normalized_sentences_religion))*100\n",
    "    print ('%-16s' % topword[0], '%-16s' % topword[1],'%-16s' %  percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2578335",
   "metadata": {},
   "source": [
    "# Religion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd4b7ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  17145           \n",
      "Number of unique words 6081            \n",
      "Average word length 6.614289880431612\n",
      "Average sentence length in characters 12.228579760863225\n",
      "Longest word     Preparation-Inquirers'\n"
     ]
    }
   ],
   "source": [
    "sentence_words = list(itertools.chain(*normalized_sentences_religion))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05fe2a2d",
   "metadata": {},
   "source": [
    "# News"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e658684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  50351           \n",
      "Number of unique words 13970           \n",
      "Average word length 6.407380191058768\n",
      "Average sentence length in characters 11.814760382117536\n",
      "Longest word     Scotch-Irish-Scandinavian\n"
     ]
    }
   ],
   "source": [
    "sentence_words = list(itertools.chain(*normalized_sentences_news))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e6e6a",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § توکنایزیشن  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac1feb67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'delta',\n",
       " 'variant',\n",
       " 'of',\n",
       " 'COVID-19',\n",
       " 'is',\n",
       " 'not',\n",
       " 'done',\n",
       " 'with',\n",
       " 'the',\n",
       " 'U.S.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample\n",
    "\n",
    "text = 'The delta variant of COVID-19 is not done with the U.S.'\n",
    "\n",
    "# http://stackoverflow.com/questions/36353125/nltk-regular-expression-tokenizer\n",
    "pattern = r'''(?x)          # set flag to allow verbose regexps\n",
    "        (?:[A-Z]\\.)+        # abbreviations, e.g. U.S.A.\n",
    "      | \\w+(?:-\\w+)*        # words with optional internal hyphens\n",
    "      | \\$?\\d+(?:\\.\\d+)?%?\\s?  # currency and percentages, e.g. $12.40, 82%\n",
    "      | \\.\\.\\.              # ellipsis\n",
    "      | [][.,;\"'?():_`-]    # these are separate tokens; includes ], [\n",
    "    '''\n",
    "\n",
    "pattern = re.compile(pattern)\n",
    "\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca68968d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['result',\n",
       "  'although',\n",
       "  'still',\n",
       "  'make',\n",
       "  'use',\n",
       "  'distinction',\n",
       "  'much',\n",
       "  'confusion',\n",
       "  'meaning',\n",
       "  'basic',\n",
       "  'terms',\n",
       "  'employed'],\n",
       " ['meant', 'spirit', 'matter'],\n",
       " ['terms',\n",
       "  'generally',\n",
       "  'taken',\n",
       "  'granted',\n",
       "  'though',\n",
       "  'referred',\n",
       "  'direct',\n",
       "  'axiomatic',\n",
       "  'elements',\n",
       "  'common',\n",
       "  'experience'],\n",
       " ['Yet', 'contemporary', 'context', 'precisely', 'one', 'must'],\n",
       " ['modern',\n",
       "  'world',\n",
       "  'neither',\n",
       "  'spirit',\n",
       "  'matter',\n",
       "  'refer',\n",
       "  'generally',\n",
       "  'agreed-upon',\n",
       "  'elements',\n",
       "  'experience'],\n",
       " ['transitional',\n",
       "  'stage',\n",
       "  'many',\n",
       "  'connotations',\n",
       "  'former',\n",
       "  'usage',\n",
       "  'revised',\n",
       "  'rejected'],\n",
       " ['words',\n",
       "  'used',\n",
       "  'never',\n",
       "  'sure',\n",
       "  'traditional',\n",
       "  'meanings',\n",
       "  'user',\n",
       "  'may',\n",
       "  'mind',\n",
       "  'extent',\n",
       "  'revisions',\n",
       "  'rejections',\n",
       "  'former',\n",
       "  'understandings',\n",
       "  'correspond'],\n",
       " ['One',\n",
       "  'widespread',\n",
       "  'features',\n",
       "  'contemporary',\n",
       "  'thought',\n",
       "  'almost',\n",
       "  'universal',\n",
       "  'disbelief',\n",
       "  'reality',\n",
       "  'spirit'],\n",
       " ['centuries',\n",
       "  'ago',\n",
       "  'world',\n",
       "  'spirits',\n",
       "  'populous',\n",
       "  'real',\n",
       "  'world',\n",
       "  'material',\n",
       "  'entities'],\n",
       " ['popular', 'thought', 'highly', 'educated', 'well', 'true']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_sentences_religion[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abf8eb3",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § جمله‌بندی  </h2> \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4daaa0d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi Tom, how are you?\n",
      "I’m fine, thanks.\n",
      "How are you?\n",
      "Great!\n",
      "thank you.\n",
      "Hi Tom, how are you?\n",
      "I’m fine, thanks.\n",
      "How are you?\n",
      "Great!\n",
      "thank you.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example in NLTK\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text=\"\"\"Hi Tom, how are you? I’m fine, thanks. How are you? Great! thank you.\"\"\"\n",
    "\n",
    "sents=sent_tokenize(text)\n",
    "for x in sents:\n",
    "    print(x)\n",
    "\n",
    "# Explore Spacy\n",
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') # or whatever model you have installed\n",
    "doc = nlp(text)\n",
    "sents = [sent for sent in doc.sents]\n",
    "for x in sents:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e11d11c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h2 style='direction:rtl;'> § تحلیل بسامد  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b79f6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "all_tokens_religion = list(itertools.chain(*normalized_sentences_religion))\n",
    "all_tokens_news = list(itertools.chain(*normalized_sentences_news))\n",
    "\n",
    "\n",
    "dataframe = {}\n",
    "\n",
    "for opt in ['religion', 'news']:\n",
    "     dataframe[opt] = FreqDist(eval(F\"all_tokens_{opt}\")).most_common(50)\n",
    "\n",
    "freq_analysis = pd.DataFrame(dataframe)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f4cfe4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>religion</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(God, 131)</td>\n",
       "      <td>(said, 402)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(world, 90)</td>\n",
       "      <td>(Mrs., 253)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(one, 87)</td>\n",
       "      <td>(would, 244)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(may, 78)</td>\n",
       "      <td>(one, 184)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(new, 77)</td>\n",
       "      <td>(Mr., 170)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(would, 68)</td>\n",
       "      <td>(last, 161)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(man, 64)</td>\n",
       "      <td>(two, 157)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(could, 59)</td>\n",
       "      <td>(new, 148)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(Christ, 59)</td>\n",
       "      <td>(first, 143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(also, 56)</td>\n",
       "      <td>(year, 138)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(life, 55)</td>\n",
       "      <td>(home, 127)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(must, 54)</td>\n",
       "      <td>(also, 120)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(church, 51)</td>\n",
       "      <td>(made, 107)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(Christian, 50)</td>\n",
       "      <td>(years, 102)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(power, 49)</td>\n",
       "      <td>(time, 97)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(members, 49)</td>\n",
       "      <td>(three, 97)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(spirit, 46)</td>\n",
       "      <td>(New, 93)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(many, 46)</td>\n",
       "      <td>(state, 90)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(human, 43)</td>\n",
       "      <td>(President, 89)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(Church, 43)</td>\n",
       "      <td>(week, 86)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(death, 41)</td>\n",
       "      <td>(could, 86)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(faith, 40)</td>\n",
       "      <td>(four, 73)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(say, 39)</td>\n",
       "      <td>(man, 72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(even, 39)</td>\n",
       "      <td>(House, 71)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(men, 39)</td>\n",
       "      <td>(back, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(good, 37)</td>\n",
       "      <td>(members, 69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(years, 37)</td>\n",
       "      <td>(American, 67)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(England, 37)</td>\n",
       "      <td>(may, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(still, 35)</td>\n",
       "      <td>(program, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(see, 35)</td>\n",
       "      <td>(work, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(time, 34)</td>\n",
       "      <td>(get, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(people, 34)</td>\n",
       "      <td>(Kennedy, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(Catholic, 34)</td>\n",
       "      <td>(John, 65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(Jesus, 33)</td>\n",
       "      <td>(school, 65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(number, 33)</td>\n",
       "      <td>(night, 64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(membership, 33)</td>\n",
       "      <td>(State, 63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(born, 33)</td>\n",
       "      <td>(meeting, 62)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(way, 31)</td>\n",
       "      <td>(since, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(know, 30)</td>\n",
       "      <td>(per, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(John, 30)</td>\n",
       "      <td>(day, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(sin, 30)</td>\n",
       "      <td>(many, 60)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(action, 30)</td>\n",
       "      <td>(U.S., 58)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(churches, 29)</td>\n",
       "      <td>(United, 58)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(much, 28)</td>\n",
       "      <td>(yesterday, 56)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(made, 28)</td>\n",
       "      <td>(Monday, 54)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(St., 28)</td>\n",
       "      <td>(tax, 53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(Parker, 28)</td>\n",
       "      <td>(told, 53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(experience, 27)</td>\n",
       "      <td>(president, 53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>(two, 27)</td>\n",
       "      <td>(even, 53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>(said, 27)</td>\n",
       "      <td>(administration, 52)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            religion                  news\n",
       "0         (God, 131)           (said, 402)\n",
       "1        (world, 90)           (Mrs., 253)\n",
       "2          (one, 87)          (would, 244)\n",
       "3          (may, 78)            (one, 184)\n",
       "4          (new, 77)            (Mr., 170)\n",
       "5        (would, 68)           (last, 161)\n",
       "6          (man, 64)            (two, 157)\n",
       "7        (could, 59)            (new, 148)\n",
       "8       (Christ, 59)          (first, 143)\n",
       "9         (also, 56)           (year, 138)\n",
       "10        (life, 55)           (home, 127)\n",
       "11        (must, 54)           (also, 120)\n",
       "12      (church, 51)           (made, 107)\n",
       "13   (Christian, 50)          (years, 102)\n",
       "14       (power, 49)            (time, 97)\n",
       "15     (members, 49)           (three, 97)\n",
       "16      (spirit, 46)             (New, 93)\n",
       "17        (many, 46)           (state, 90)\n",
       "18       (human, 43)       (President, 89)\n",
       "19      (Church, 43)            (week, 86)\n",
       "20       (death, 41)           (could, 86)\n",
       "21       (faith, 40)            (four, 73)\n",
       "22         (say, 39)             (man, 72)\n",
       "23        (even, 39)           (House, 71)\n",
       "24         (men, 39)            (back, 69)\n",
       "25        (good, 37)         (members, 69)\n",
       "26       (years, 37)        (American, 67)\n",
       "27     (England, 37)             (may, 66)\n",
       "28       (still, 35)         (program, 66)\n",
       "29         (see, 35)            (work, 66)\n",
       "30        (time, 34)             (get, 66)\n",
       "31      (people, 34)         (Kennedy, 66)\n",
       "32    (Catholic, 34)            (John, 65)\n",
       "33       (Jesus, 33)          (school, 65)\n",
       "34      (number, 33)           (night, 64)\n",
       "35  (membership, 33)           (State, 63)\n",
       "36        (born, 33)         (meeting, 62)\n",
       "37         (way, 31)           (since, 61)\n",
       "38        (know, 30)             (per, 61)\n",
       "39        (John, 30)             (day, 61)\n",
       "40         (sin, 30)            (many, 60)\n",
       "41      (action, 30)            (U.S., 58)\n",
       "42    (churches, 29)          (United, 58)\n",
       "43        (much, 28)       (yesterday, 56)\n",
       "44        (made, 28)          (Monday, 54)\n",
       "45         (St., 28)             (tax, 53)\n",
       "46      (Parker, 28)            (told, 53)\n",
       "47  (experience, 27)       (president, 53)\n",
       "48         (two, 27)            (even, 53)\n",
       "49        (said, 27)  (administration, 52)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073ccc77",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h2 style='direction:rtl;'> § استفاده از lemmatization, stemming  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d8854a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer  = PorterStemmer()\n",
    "\n",
    "\n",
    "def get_lemma_set(tok, opt=1):\n",
    "    if opt ==1:\n",
    "        return stemmer.stem(tok)\n",
    "    if opt ==2:\n",
    "        return lemmatizer.lemmatize(tok)\n",
    "    if opt ==3:\n",
    "        # write your own\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9290ab12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 17145/17145 [00:01<00:00, 12446.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50351/50351 [00:00<00:00, 271495.51it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>religion</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(God, 131)</td>\n",
       "      <td>(said, 402)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(one, 93)</td>\n",
       "      <td>(Mrs., 253)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(world, 91)</td>\n",
       "      <td>(would, 244)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(church, 80)</td>\n",
       "      <td>(year, 240)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(may, 78)</td>\n",
       "      <td>(one, 192)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(new, 77)</td>\n",
       "      <td>(Mr., 170)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(would, 68)</td>\n",
       "      <td>(last, 161)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(man, 64)</td>\n",
       "      <td>(two, 157)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(life, 62)</td>\n",
       "      <td>(new, 148)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(power, 60)</td>\n",
       "      <td>(first, 143)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(could, 59)</td>\n",
       "      <td>(home, 136)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(member, 59)</td>\n",
       "      <td>(also, 120)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Christ, 59)</td>\n",
       "      <td>(time, 115)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(spirit, 58)</td>\n",
       "      <td>(state, 112)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(also, 56)</td>\n",
       "      <td>(week, 111)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(must, 54)</td>\n",
       "      <td>(made, 107)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(number, 54)</td>\n",
       "      <td>(member, 104)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(Christian, 50)</td>\n",
       "      <td>(school, 102)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(say, 48)</td>\n",
       "      <td>(day, 99)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(many, 46)</td>\n",
       "      <td>(three, 97)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(year, 45)</td>\n",
       "      <td>(New, 93)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(human, 44)</td>\n",
       "      <td>(President, 89)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(Church, 43)</td>\n",
       "      <td>(could, 86)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(way, 42)</td>\n",
       "      <td>(program, 78)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(mean, 42)</td>\n",
       "      <td>(work, 74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(death, 41)</td>\n",
       "      <td>(get, 74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(thing, 40)</td>\n",
       "      <td>(month, 74)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(faith, 40)</td>\n",
       "      <td>(four, 73)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(time, 39)</td>\n",
       "      <td>(game, 73)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(even, 39)</td>\n",
       "      <td>(man, 72)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(men, 39)</td>\n",
       "      <td>(House, 71)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(see, 38)</td>\n",
       "      <td>(back, 70)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(good, 37)</td>\n",
       "      <td>(law, 68)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(England, 37)</td>\n",
       "      <td>(car, 68)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(people, 36)</td>\n",
       "      <td>(plan, 67)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(still, 35)</td>\n",
       "      <td>(meeting, 67)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(know, 35)</td>\n",
       "      <td>(American, 67)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(sin, 35)</td>\n",
       "      <td>(may, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(fear, 34)</td>\n",
       "      <td>(Kennedy, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(need, 34)</td>\n",
       "      <td>(night, 65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(Catholic, 34)</td>\n",
       "      <td>(run, 65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>(make, 33)</td>\n",
       "      <td>(John, 65)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>(word, 33)</td>\n",
       "      <td>(State, 63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>(Jesus, 33)</td>\n",
       "      <td>(tax, 63)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>(membership, 33)</td>\n",
       "      <td>(since, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>(born, 33)</td>\n",
       "      <td>(per, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>(work, 32)</td>\n",
       "      <td>(bill, 61)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>(atom, 32)</td>\n",
       "      <td>(many, 60)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>(experience, 31)</td>\n",
       "      <td>(company, 60)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>(John, 30)</td>\n",
       "      <td>(sale, 59)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            religion             news\n",
       "0         (God, 131)      (said, 402)\n",
       "1          (one, 93)      (Mrs., 253)\n",
       "2        (world, 91)     (would, 244)\n",
       "3       (church, 80)      (year, 240)\n",
       "4          (may, 78)       (one, 192)\n",
       "5          (new, 77)       (Mr., 170)\n",
       "6        (would, 68)      (last, 161)\n",
       "7          (man, 64)       (two, 157)\n",
       "8         (life, 62)       (new, 148)\n",
       "9        (power, 60)     (first, 143)\n",
       "10       (could, 59)      (home, 136)\n",
       "11      (member, 59)      (also, 120)\n",
       "12      (Christ, 59)      (time, 115)\n",
       "13      (spirit, 58)     (state, 112)\n",
       "14        (also, 56)      (week, 111)\n",
       "15        (must, 54)      (made, 107)\n",
       "16      (number, 54)    (member, 104)\n",
       "17   (Christian, 50)    (school, 102)\n",
       "18         (say, 48)        (day, 99)\n",
       "19        (many, 46)      (three, 97)\n",
       "20        (year, 45)        (New, 93)\n",
       "21       (human, 44)  (President, 89)\n",
       "22      (Church, 43)      (could, 86)\n",
       "23         (way, 42)    (program, 78)\n",
       "24        (mean, 42)       (work, 74)\n",
       "25       (death, 41)        (get, 74)\n",
       "26       (thing, 40)      (month, 74)\n",
       "27       (faith, 40)       (four, 73)\n",
       "28        (time, 39)       (game, 73)\n",
       "29        (even, 39)        (man, 72)\n",
       "30         (men, 39)      (House, 71)\n",
       "31         (see, 38)       (back, 70)\n",
       "32        (good, 37)        (law, 68)\n",
       "33     (England, 37)        (car, 68)\n",
       "34      (people, 36)       (plan, 67)\n",
       "35       (still, 35)    (meeting, 67)\n",
       "36        (know, 35)   (American, 67)\n",
       "37         (sin, 35)        (may, 66)\n",
       "38        (fear, 34)    (Kennedy, 66)\n",
       "39        (need, 34)      (night, 65)\n",
       "40    (Catholic, 34)        (run, 65)\n",
       "41        (make, 33)       (John, 65)\n",
       "42        (word, 33)      (State, 63)\n",
       "43       (Jesus, 33)        (tax, 63)\n",
       "44  (membership, 33)      (since, 61)\n",
       "45        (born, 33)        (per, 61)\n",
       "46        (work, 32)       (bill, 61)\n",
       "47        (atom, 32)       (many, 60)\n",
       "48  (experience, 31)    (company, 60)\n",
       "49        (John, 30)       (sale, 59)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt = 2\n",
    "\n",
    "religion_tokens_nonstop_lemstem =   [get_lemma_set(t, opt) for t in tqdm.tqdm(all_tokens_religion)]\n",
    "news_tokens_nonstop_lemstem = [get_lemma_set(t, opt) for t in tqdm.tqdm(all_tokens_news)]\n",
    "\n",
    "dataframe_nonstop_lemstem = {}\n",
    "\n",
    "for opt in ['religion', 'news']:\n",
    "     dataframe_nonstop_lemstem[opt] = FreqDist(eval(F\"{opt}_tokens_nonstop_lemstem\")).most_common(50)\n",
    "\n",
    "freq_analysis_nonstop_lemstem = pd.DataFrame(dataframe_nonstop_lemstem)   \n",
    "freq_analysis_nonstop_lemstem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe821bc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "<h2 style='direction:rtl;'> § استفاده از POS-tags  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "318f56dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f03a8a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_sets(sentences):\n",
    "    size = int(len(sentences) * 0.9)\n",
    "    train_sents = sentences[:size]\n",
    "    test_sents = sentences[size:]\n",
    "    return train_sents, test_sents\n",
    "\n",
    "def build_backoff_tagger (train_sents):\n",
    "    t0 = nltk.DefaultTagger('NN')\n",
    "    t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "    t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "    return t2\n",
    "\n",
    "\n",
    "def train_tagger(already_tagged_sents):\n",
    "    train_sents, test_sents = create_data_sets(already_tagged_sents)\n",
    "    ngram_tagger = build_backoff_tagger(train_sents)\n",
    "    print (\"%0.3f pos accuracy on test set\" % ngram_tagger.evaluate(test_sents))\n",
    "    return ngram_tagger    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "df9c4250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.911 pos accuracy on test set\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', 'government', 'hobbies',\n",
    "    'humor', 'learned', 'lore', 'mystery', 'religion', 'reviews', 'romance','science_fiction'])\n",
    "brown_tagger = train_tagger(brown_tagged_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "45e1478a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fulton', 'NP'),\n",
       " ('County', 'NN-TL'),\n",
       " ('Grand', 'JJ-TL'),\n",
       " ('Jury', 'NN-TL'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NR'),\n",
       " ('investigation', 'NN'),\n",
       " (\"Atlanta's\", 'NN'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'JJ'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBN'),\n",
       " ('evidence', 'NN'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_tagger.tag(globals()[F'normalized_sentences_{opt}'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "77129289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Fulton', 'NNP'),\n",
       " ('County', 'NNP'),\n",
       " ('Grand', 'NNP'),\n",
       " ('Jury', 'NNP'),\n",
       " ('said', 'VBD'),\n",
       " ('Friday', 'NNP'),\n",
       " ('investigation', 'NN'),\n",
       " (\"Atlanta's\", 'NNP'),\n",
       " ('recent', 'JJ'),\n",
       " ('primary', 'JJ'),\n",
       " ('election', 'NN'),\n",
       " ('produced', 'VBD'),\n",
       " ('evidence', 'NN'),\n",
       " ('irregularities', 'NNS'),\n",
       " ('took', 'VBD'),\n",
       " ('place', 'NN')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(globals()[F'normalized_sentences_{opt}'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3dba843f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 15.87it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "dataframe_nonstop_lemstem_advjj = {}\n",
    "\n",
    "for opt in tqdm.tqdm(['religion', 'news']):\n",
    "    selected = []\n",
    "    for sentence in globals()[F'normalized_sentences_{opt}']:\n",
    "        tagged_sentence = brown_tagger.tag(sentence)\n",
    "        for w, pos in tagged_sentence:\n",
    "            if pos=='JJ':\n",
    "                selected.append(w) \n",
    "    dataframe_nonstop_lemstem_advjj[opt]= list(FreqDist(selected).most_common(40))\n",
    "\n",
    "dataframe_nonstop_lemstem_advjj = pd.DataFrame(dataframe_nonstop_lemstem_advjj)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2d3435c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>religion</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(new, 77)</td>\n",
       "      <td>(new, 148)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(Christian, 48)</td>\n",
       "      <td>(American, 66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(human, 43)</td>\n",
       "      <td>(high, 52)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Catholic, 33)</td>\n",
       "      <td>(public, 50)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(good, 29)</td>\n",
       "      <td>(good, 49)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(religious, 27)</td>\n",
       "      <td>(special, 41)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(great, 26)</td>\n",
       "      <td>(local, 39)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(social, 26)</td>\n",
       "      <td>(big, 38)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(spiritual, 25)</td>\n",
       "      <td>(federal, 35)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(moral, 21)</td>\n",
       "      <td>(young, 35)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(real, 20)</td>\n",
       "      <td>(long, 34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(modern, 18)</td>\n",
       "      <td>(annual, 33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(Chinese, 18)</td>\n",
       "      <td>(national, 32)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(important, 17)</td>\n",
       "      <td>(military, 30)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(American, 17)</td>\n",
       "      <td>(great, 30)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(certain, 17)</td>\n",
       "      <td>(foreign, 29)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(whole, 15)</td>\n",
       "      <td>(possible, 28)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(possible, 15)</td>\n",
       "      <td>(political, 27)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(political, 15)</td>\n",
       "      <td>(small, 27)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(evil, 14)</td>\n",
       "      <td>(open, 26)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(Protestant, 14)</td>\n",
       "      <td>(major, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(personal, 14)</td>\n",
       "      <td>(large, 25)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(natural, 14)</td>\n",
       "      <td>(able, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(magic, 14)</td>\n",
       "      <td>(British, 24)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(traditional, 13)</td>\n",
       "      <td>(old, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(English, 13)</td>\n",
       "      <td>(international, 23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(small, 13)</td>\n",
       "      <td>(final, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(true, 12)</td>\n",
       "      <td>(Catholic, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(Roman, 12)</td>\n",
       "      <td>(anti-trust, 21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(entire, 12)</td>\n",
       "      <td>(recent, 20)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(theological, 11)</td>\n",
       "      <td>(private, 19)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(public, 11)</td>\n",
       "      <td>(medical, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(anti-slavery, 11)</td>\n",
       "      <td>(certain, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(large, 10)</td>\n",
       "      <td>(complete, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(individual, 10)</td>\n",
       "      <td>(necessary, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(recent, 10)</td>\n",
       "      <td>(full, 18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(professional, 10)</td>\n",
       "      <td>(similar, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(nuclear, 10)</td>\n",
       "      <td>(real, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(common, 9)</td>\n",
       "      <td>(social, 17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(popular, 9)</td>\n",
       "      <td>(fine, 16)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              religion                 news\n",
       "0            (new, 77)           (new, 148)\n",
       "1      (Christian, 48)       (American, 66)\n",
       "2          (human, 43)           (high, 52)\n",
       "3       (Catholic, 33)         (public, 50)\n",
       "4           (good, 29)           (good, 49)\n",
       "5      (religious, 27)        (special, 41)\n",
       "6          (great, 26)          (local, 39)\n",
       "7         (social, 26)            (big, 38)\n",
       "8      (spiritual, 25)        (federal, 35)\n",
       "9          (moral, 21)          (young, 35)\n",
       "10          (real, 20)           (long, 34)\n",
       "11        (modern, 18)         (annual, 33)\n",
       "12       (Chinese, 18)       (national, 32)\n",
       "13     (important, 17)       (military, 30)\n",
       "14      (American, 17)          (great, 30)\n",
       "15       (certain, 17)        (foreign, 29)\n",
       "16         (whole, 15)       (possible, 28)\n",
       "17      (possible, 15)      (political, 27)\n",
       "18     (political, 15)          (small, 27)\n",
       "19          (evil, 14)           (open, 26)\n",
       "20    (Protestant, 14)          (major, 25)\n",
       "21      (personal, 14)          (large, 25)\n",
       "22       (natural, 14)           (able, 24)\n",
       "23         (magic, 14)        (British, 24)\n",
       "24   (traditional, 13)            (old, 23)\n",
       "25       (English, 13)  (international, 23)\n",
       "26         (small, 13)          (final, 21)\n",
       "27          (true, 12)       (Catholic, 21)\n",
       "28         (Roman, 12)     (anti-trust, 21)\n",
       "29        (entire, 12)         (recent, 20)\n",
       "30   (theological, 11)        (private, 19)\n",
       "31        (public, 11)        (medical, 18)\n",
       "32  (anti-slavery, 11)        (certain, 18)\n",
       "33         (large, 10)       (complete, 18)\n",
       "34    (individual, 10)      (necessary, 18)\n",
       "35        (recent, 10)           (full, 18)\n",
       "36  (professional, 10)        (similar, 17)\n",
       "37       (nuclear, 10)           (real, 17)\n",
       "38         (common, 9)         (social, 17)\n",
       "39        (popular, 9)           (fine, 16)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_nonstop_lemstem_advjj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36859470",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § دیدن سیاق  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73901a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 25 matches:\n",
      " spirit matter term generally taken granted \n",
      "cisely one must modern world neither spirit matter refer generally agreed-upon e\n",
      "t almost universal disbelief reality spirit century ago world spirit populous re\n",
      "ief reality spirit century ago world spirit populous real world material entity \n",
      "man rock tree star world word matter spirit referred directly known reality comm\n",
      "ategory reality large extent emptied spirit world entity previously populated ca\n",
      "aim due credit objectification world spirit popular superstition certainly gone \n",
      "certainly gone far beyond experience spirit could justify support Science fully \n",
      "ty popular imagination peopled world spirit entity soon lost whatever status enj\n",
      "nmixed blessing scientific debunking spirit world way successful thorough house \n",
      "y inadequate mean dealing experience spirit Although particular form conceptuali\n",
      "imagination made response experience spirit undoubtedly defective raw experience\n",
      " leaf play spell kind spell exposure spirit living active manifestation always e\n",
      "in terrible power better answer give spirit attempt say spirit employ commonly u\n",
      "etter answer give spirit attempt say spirit employ commonly used word designate \n",
      " mind Hale's still convinced society spirit beyond ken page hand little later sa\n",
      "s come watch play like spell reality spirit emerges play spite author's convicti\n",
      "y spite author's conviction contrary Spirit community nothing whole range human \n",
      "rience widely known universally felt spirit Apart spirit could community spirit \n",
      " known universally felt spirit Apart spirit could community spirit draw men comm\n",
      " spirit Apart spirit could community spirit draw men community give community un\n",
      "ohesiveness permanence Think example spirit Marine Corps Surely reality acknowle\n",
      "ctive reality experienced known many spirit know spirit Nazism Communism school \n",
      "y experienced known many spirit know spirit Nazism Communism school spirit spiri\n",
      " know spirit Nazism Communism school spirit spirit street corner gang football t\n"
     ]
    }
   ],
   "source": [
    "hafez_text = nltk.Text(religion_tokens_nonstop_lemstem)\n",
    "hafez_text.concordance('Spirit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ec10a0",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § عبارت یابی  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cef5df48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1789-Washington.txt', '1793-Washington.txt', '1797-Adams.txt', '1801-Jefferson.txt', '1805-Jefferson.txt', '1809-Madison.txt', '1813-Madison.txt', '1817-Monroe.txt', '1821-Monroe.txt', '1825-Adams.txt', '1829-Jackson.txt', '1833-Jackson.txt', '1837-VanBuren.txt', '1841-Harrison.txt', '1845-Polk.txt', '1849-Taylor.txt', '1853-Pierce.txt', '1857-Buchanan.txt', '1861-Lincoln.txt', '1865-Lincoln.txt', '1869-Grant.txt', '1873-Grant.txt', '1877-Hayes.txt', '1881-Garfield.txt', '1885-Cleveland.txt', '1889-Harrison.txt', '1893-Cleveland.txt', '1897-McKinley.txt', '1901-McKinley.txt', '1905-Roosevelt.txt', '1909-Taft.txt', '1913-Wilson.txt', '1917-Wilson.txt', '1921-Harding.txt', '1925-Coolidge.txt', '1929-Hoover.txt', '1933-Roosevelt.txt', '1937-Roosevelt.txt', '1941-Roosevelt.txt', '1945-Roosevelt.txt', '1949-Truman.txt', '1953-Eisenhower.txt', '1957-Eisenhower.txt', '1961-Kennedy.txt', '1965-Johnson.txt', '1969-Nixon.txt', '1973-Nixon.txt', '1977-Carter.txt', '1981-Reagan.txt', '1985-Reagan.txt', '1989-Bush.txt', '1993-Clinton.txt', '1997-Clinton.txt', '2001-Bush.txt', '2005-Bush.txt', '2009-Obama.txt', '2013-Obama.txt', '2017-Trump.txt', '2021-Biden.txt']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import inaugural\n",
    "#nltk.download('inaugural')\n",
    "print(inaugural.fileids())\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9205c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "Lincoln = list(inaugural.words(inaugural.fileids()[18]))\n",
    "Obama = list(inaugural.words(inaugural.fileids()[56]))\n",
    "Trump = list(inaugural.words(inaugural.fileids()[57]))\n",
    "\n",
    "Lincoln =' '.join(Lincoln)\n",
    "Lincoln_sentences =  sent_tokenize(Lincoln)\n",
    "Lincoln_sentences =  [x.split() for x in Lincoln_sentences]\n",
    "Lincoln_sentences_tagged = [brown_tagger.tag(sent) for sent in Lincoln_sentences]\n",
    "\n",
    "Obama =' '.join(Obama)\n",
    "Obama_sentences =  sent_tokenize(Obama)\n",
    "Obama_sentences =  [x.split() for x in Obama_sentences]\n",
    "Obama_sentences_tagged = [brown_tagger.tag(sent) for sent in Obama_sentences]\n",
    "\n",
    "Trump =' '.join(Trump)\n",
    "Trump_sentences =  sent_tokenize(Trump)\n",
    "Trump_sentences =  [x.split() for x in Trump_sentences]\n",
    "Trump_sentences_tagged = [brown_tagger.tag(sent) for sent in Trump_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f0f83542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  4005            \n",
      "Number of unique words 1078            \n",
      "Average word length 4.329588014981273\n",
      "Average sentence length in characters 7.659176029962547\n",
      "Longest word     unconstitutional\n"
     ]
    }
   ],
   "source": [
    "sentence_words = list(itertools.chain(*Lincoln_sentences))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ab2cb930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  2369            \n",
      "Number of unique words 820             \n",
      "Average word length 4.149430139299282\n",
      "Average sentence length in characters 7.298860278598565\n",
      "Longest word     responsibility  \n"
     ]
    }
   ],
   "source": [
    "sentence_words = list(itertools.chain(*Obama_sentences))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad9de0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words  1693            \n",
      "Number of unique words 582             \n",
      "Average word length 4.112226816302422\n",
      "Average sentence length in characters 7.224453632604844\n",
      "Longest word     administration  \n"
     ]
    }
   ],
   "source": [
    "sentence_words = list(itertools.chain(*Trump_sentences))\n",
    "print ('%-16s' % 'Number of words', '%-16s' % len(sentence_words))\n",
    "print ('%-16s' % 'Number of unique words', '%-16s' % len(set(sentence_words)))\n",
    "avg=np.sum([len(word) for word in sentence_words])/len(sentence_words)\n",
    "print ('%-16s' % 'Average word length', '%-16s' % avg)\n",
    "avg_sentence_length_c=np.mean([len(' '.join(sentence)) for sentence in sentence_words])\n",
    "print ('%-16s' % 'Average sentence length in characters', '%-16s' % avg_sentence_length_c)\n",
    "print ('%-16s' % 'Longest word', '%-16s' % sentence_words[np.argmax([len(word) for word in sentence_words])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "87ccf1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkTechnicalTerm(sentence):\n",
    "    grammar = r\"\"\"\n",
    "      TECHTERM: {<JJ|NN>+<NN|CD>|<NN>}\n",
    "    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar)\n",
    "    return (cp.parse(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7165f046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 83.18it/s]\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "technical_terms = {}\n",
    "opt_technical_terms = {}\n",
    "\n",
    "\n",
    "\n",
    "for opt in tqdm.tqdm(['Lincoln', 'Obama','Trump']):\n",
    "    technical_terms[opt] = []\n",
    "    tagged = globals()[F'{opt}_sentences_tagged']\n",
    "    for sentence in tagged:\n",
    "        tree=chunkTechnicalTerm(sentence)\n",
    "        for subtree in tree.subtrees():\n",
    "            if subtree.label() == 'TECHTERM':\n",
    "                technical_terms[opt].append(subtree.leaves())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "699d1ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 4572.28it/s]\n"
     ]
    }
   ],
   "source": [
    "for opt in tqdm.tqdm(['Lincoln', 'Obama','Trump']):\n",
    "    opt_technical_terms[opt]=FreqDist([' '.join([x for x,y in sent]) for sent in technical_terms[opt] if len(sent)>1]).most_common(40)    \n",
    "                                   \n",
    "                                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3c772fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Lincoln</th>\n",
       "      <th>Obama</th>\n",
       "      <th>Trump</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(such service, 2)</td>\n",
       "      <td>(exceptionalâ , 1)</td>\n",
       "      <td>(transferring power, 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(organic law, 2)</td>\n",
       "      <td>(Americanâ , 1)</td>\n",
       "      <td>(great national effort, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(constitutional right, 2)</td>\n",
       "      <td>(modern economy, 1)</td>\n",
       "      <td>(peaceful transfer, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(foreign slave trade, 2)</td>\n",
       "      <td>(free market, 1)</td>\n",
       "      <td>(Michelle Obama, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(whole subject, 2)</td>\n",
       "      <td>(fair play, 1)</td>\n",
       "      <td>(gracious aid, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>(office .\", 1)</td>\n",
       "      <td>(great nation, 1)</td>\n",
       "      <td>(s ceremony, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>(special anxiety, 1)</td>\n",
       "      <td>(central authority, 1)</td>\n",
       "      <td>(special meaning, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>(personal security, 1)</td>\n",
       "      <td>(hard work, 1)</td>\n",
       "      <td>(small group, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>(reasonable cause, 1)</td>\n",
       "      <td>(personal responsibility, 1)</td>\n",
       "      <td>(historic movement, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(such apprehension, 1)</td>\n",
       "      <td>(collective action, 1)</td>\n",
       "      <td>(crucial conviction, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>(ample evidence, 1)</td>\n",
       "      <td>(s world, 1)</td>\n",
       "      <td>(different reality, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>(lawful right, 1)</td>\n",
       "      <td>(research labs, 1)</td>\n",
       "      <td>(education system, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>(full knowledge, 1)</td>\n",
       "      <td>(economic recovery, 1)</td>\n",
       "      <td>(unrealized potential, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>(emphatic resolution, 1)</td>\n",
       "      <td>(endless capacity, 1)</td>\n",
       "      <td>(glorious destiny, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>(own judgment, 1)</td>\n",
       "      <td>(itâ , 1)</td>\n",
       "      <td>(foreign industry, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>(political fabric, 1)</td>\n",
       "      <td>(s prosperity, 1)</td>\n",
       "      <td>(American industry, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>(lawless invasion, 1)</td>\n",
       "      <td>(middle class, 1)</td>\n",
       "      <td>(sad depletion, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>(public attention, 1)</td>\n",
       "      <td>(honest labor, 1)</td>\n",
       "      <td>(s infrastructure, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>(conclusive evidence, 1)</td>\n",
       "      <td>(little girl, 1)</td>\n",
       "      <td>(rich while, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>(clause \", 1)</td>\n",
       "      <td>(bleakest poverty, 1)</td>\n",
       "      <td>(middle class, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>(good temper, 1)</td>\n",
       "      <td>(real meaning, 1)</td>\n",
       "      <td>(new decree, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>(equal unanimity frame, 1)</td>\n",
       "      <td>(basic measure, 1)</td>\n",
       "      <td>(foreign capital, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>(unanimous oath, 1)</td>\n",
       "      <td>(health care, 1)</td>\n",
       "      <td>(new vision, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>(material one, 1)</td>\n",
       "      <td>(country freedom, 1)</td>\n",
       "      <td>(great prosperity, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>(little consequence, 1)</td>\n",
       "      <td>(job loss, 1)</td>\n",
       "      <td>(American labor, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>(unsubstantial controversy, 1)</td>\n",
       "      <td>(sudden illness, 1)</td>\n",
       "      <td>(good will, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>(humane jurisprudence, 1)</td>\n",
       "      <td>(terrible storm, 1)</td>\n",
       "      <td>(exampleâ , 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>(free man, 1)</td>\n",
       "      <td>(overwhelming judgment, 1)</td>\n",
       "      <td>(shineâ , 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>(official oath, 1)</td>\n",
       "      <td>(sustainable energy, 1)</td>\n",
       "      <td>(radical Islamic terrorism, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>(period fifteen, 1)</td>\n",
       "      <td>(economic vitality, 1)</td>\n",
       "      <td>(total allegiance, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>(executive branch, 1)</td>\n",
       "      <td>(national treasureâ , 1)</td>\n",
       "      <td>(unity .\", 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>(great success, 1)</td>\n",
       "      <td>(perpetual war, 1)</td>\n",
       "      <td>(law enforcement, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>(brief constitutional term, 1)</td>\n",
       "      <td>(friendsâ , 1)</td>\n",
       "      <td>(empty talk, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>(peculiar difficulty, 1)</td>\n",
       "      <td>(peacefullyâ , 1)</td>\n",
       "      <td>(new millennium, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>(universal law, 1)</td>\n",
       "      <td>(peaceful world, 1)</td>\n",
       "      <td>(new national pride, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(fundamental law, 1)</td>\n",
       "      <td>(powerful nation, 1)</td>\n",
       "      <td>(s time, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>(own termination, 1)</td>\n",
       "      <td>(prejudiceâ , 1)</td>\n",
       "      <td>(old wisdom, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>(peaceably unmade, 1)</td>\n",
       "      <td>(mere charity, 1)</td>\n",
       "      <td>(red blood, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>(lawfully rescind, 1)</td>\n",
       "      <td>(constant advance, 1)</td>\n",
       "      <td>(great American flag, 1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>(legal contemplation, 1)</td>\n",
       "      <td>(common creed, 1)</td>\n",
       "      <td>(urban sprawl, 1)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Lincoln                         Obama  \\\n",
       "0                (such service, 2)          (exceptionalâ , 1)   \n",
       "1                 (organic law, 2)             (Americanâ , 1)   \n",
       "2        (constitutional right, 2)           (modern economy, 1)   \n",
       "3         (foreign slave trade, 2)              (free market, 1)   \n",
       "4               (whole subject, 2)                (fair play, 1)   \n",
       "5                   (office .\", 1)             (great nation, 1)   \n",
       "6             (special anxiety, 1)        (central authority, 1)   \n",
       "7           (personal security, 1)                (hard work, 1)   \n",
       "8            (reasonable cause, 1)  (personal responsibility, 1)   \n",
       "9           (such apprehension, 1)        (collective action, 1)   \n",
       "10             (ample evidence, 1)                  (s world, 1)   \n",
       "11               (lawful right, 1)            (research labs, 1)   \n",
       "12             (full knowledge, 1)        (economic recovery, 1)   \n",
       "13        (emphatic resolution, 1)         (endless capacity, 1)   \n",
       "14               (own judgment, 1)                   (itâ , 1)   \n",
       "15           (political fabric, 1)             (s prosperity, 1)   \n",
       "16           (lawless invasion, 1)             (middle class, 1)   \n",
       "17           (public attention, 1)             (honest labor, 1)   \n",
       "18        (conclusive evidence, 1)              (little girl, 1)   \n",
       "19                   (clause \", 1)         (bleakest poverty, 1)   \n",
       "20                (good temper, 1)             (real meaning, 1)   \n",
       "21      (equal unanimity frame, 1)            (basic measure, 1)   \n",
       "22             (unanimous oath, 1)              (health care, 1)   \n",
       "23               (material one, 1)          (country freedom, 1)   \n",
       "24         (little consequence, 1)                 (job loss, 1)   \n",
       "25  (unsubstantial controversy, 1)           (sudden illness, 1)   \n",
       "26       (humane jurisprudence, 1)           (terrible storm, 1)   \n",
       "27                   (free man, 1)    (overwhelming judgment, 1)   \n",
       "28              (official oath, 1)       (sustainable energy, 1)   \n",
       "29             (period fifteen, 1)        (economic vitality, 1)   \n",
       "30           (executive branch, 1)    (national treasureâ , 1)   \n",
       "31              (great success, 1)            (perpetual war, 1)   \n",
       "32  (brief constitutional term, 1)              (friendsâ , 1)   \n",
       "33        (peculiar difficulty, 1)           (peacefullyâ , 1)   \n",
       "34              (universal law, 1)           (peaceful world, 1)   \n",
       "35            (fundamental law, 1)          (powerful nation, 1)   \n",
       "36            (own termination, 1)            (prejudiceâ , 1)   \n",
       "37           (peaceably unmade, 1)             (mere charity, 1)   \n",
       "38           (lawfully rescind, 1)         (constant advance, 1)   \n",
       "39        (legal contemplation, 1)             (common creed, 1)   \n",
       "\n",
       "                             Trump  \n",
       "0          (transferring power, 2)  \n",
       "1       (great national effort, 1)  \n",
       "2           (peaceful transfer, 1)  \n",
       "3              (Michelle Obama, 1)  \n",
       "4                (gracious aid, 1)  \n",
       "5                  (s ceremony, 1)  \n",
       "6             (special meaning, 1)  \n",
       "7                 (small group, 1)  \n",
       "8           (historic movement, 1)  \n",
       "9          (crucial conviction, 1)  \n",
       "10          (different reality, 1)  \n",
       "11           (education system, 1)  \n",
       "12       (unrealized potential, 1)  \n",
       "13           (glorious destiny, 1)  \n",
       "14           (foreign industry, 1)  \n",
       "15          (American industry, 1)  \n",
       "16              (sad depletion, 1)  \n",
       "17           (s infrastructure, 1)  \n",
       "18                 (rich while, 1)  \n",
       "19               (middle class, 1)  \n",
       "20                 (new decree, 1)  \n",
       "21            (foreign capital, 1)  \n",
       "22                 (new vision, 1)  \n",
       "23           (great prosperity, 1)  \n",
       "24             (American labor, 1)  \n",
       "25                  (good will, 1)  \n",
       "26                (exampleâ , 1)  \n",
       "27                  (shineâ , 1)  \n",
       "28  (radical Islamic terrorism, 1)  \n",
       "29           (total allegiance, 1)  \n",
       "30                   (unity .\", 1)  \n",
       "31            (law enforcement, 1)  \n",
       "32                 (empty talk, 1)  \n",
       "33             (new millennium, 1)  \n",
       "34         (new national pride, 1)  \n",
       "35                     (s time, 1)  \n",
       "36                 (old wisdom, 1)  \n",
       "37                  (red blood, 1)  \n",
       "38        (great American flag, 1)  \n",
       "39               (urban sprawl, 1)  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt_technical_terms = pd.DataFrame(opt_technical_terms) \n",
    "opt_technical_terms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18620d1",
   "metadata": {},
   "source": [
    "<h2 style='direction:rtl;'> § WordNet  </h2> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3b7db689",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.collocations import *\n",
    "from string import *\n",
    "\n",
    "def freq_normed_unigrams(sents):\n",
    "    wnl = WordNetLemmatizer() # to get word stems\n",
    "    \n",
    "    tagged_POS_sents = [nltk.pos_tag(sent) for sent in sents] # tags sents\n",
    "    \n",
    "    normed_tagged_words = [wnl.lemmatize(word[0].lower()) for sent in tagged_POS_sents\n",
    "                           for word in sent \n",
    "                           if word[0].lower() not in nltk.corpus.stopwords.words('english')\n",
    "                           and word[0] not in punctuation # remove punctuation\n",
    "                           and not re.search(r'''^[\\.,;\"'?!():\\-_`]+$''', word[0])\n",
    "                           and word[1].startswith('N')]  # include only nouns\n",
    "\n",
    "    top_normed_unigrams = [word for (word, count) in nltk.FreqDist(normed_tagged_words).most_common(40)]\n",
    "    return top_normed_unigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67ace76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Trump_top_words = freq_normed_unigrams(Trump_sentences)\n",
    "Obama_top_words = freq_normed_unigrams(Obama_sentences)\n",
    "Lincoln_top_words = freq_normed_unigrams(Lincoln_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "457a7c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "from collections import defaultdict\n",
    "\n",
    "def categories_from_hypernyms(termlist):\n",
    "    hypterms = []\n",
    "    hypterms_dict = defaultdict()\n",
    "    for term in termlist:                  # for each term\n",
    "        s = wn.synsets(term.lower(), 'n')  # get its nominal synsets\n",
    "        for syn in s:                      # for each lemma synset\n",
    "            for hyp in syn.hypernyms():    # It has a list of hypernyms\n",
    "                hypterms.append(hyp.name())      # Extract the hypernym name and add to list\n",
    "                if hyp.name() not in hypterms_dict:\n",
    "                    hypterms_dict[hyp.name()] = list()\n",
    "                hypterms_dict[hyp.name()].append(term)  # Extract examples and add them to dict\n",
    "                \n",
    "    hypfd = nltk.FreqDist(hypterms)             # After going through all the nouns, print out the hypernyms \n",
    "    for (name, count) in hypfd.most_common(25):  # that have accumulated the most counts (have seen the most descendents)\n",
    "        print( name, '({0})'.format(count))\n",
    "        print ('\\t', ', '.join(set(hypterms_dict[name])))  # show the children found for each hypernym\n",
    "        print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "87664090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_period.n.01 (8)\n",
      "\t day, year, life\n",
      "\n",
      "people.n.01 (4)\n",
      "\t world, land, nation, country\n",
      "\n",
      "group.n.01 (4)\n",
      "\t world, people\n",
      "\n",
      "political_unit.n.01 (3)\n",
      "\t land, nation, country\n",
      "\n",
      "administrative_district.n.01 (3)\n",
      "\t city, land, country\n",
      "\n",
      "assets.n.01 (3)\n",
      "\t capital, share\n",
      "\n",
      "person.n.01 (3)\n",
      "\t life, child, party\n",
      "\n",
      "being.n.01 (3)\n",
      "\t life\n",
      "\n",
      "region.n.03 (2)\n",
      "\t land, country\n",
      "\n",
      "inhabitant.n.01 (2)\n",
      "\t american\n",
      "\n",
      "experience.n.02 (2)\n",
      "\t world, life\n",
      "\n",
      "head_of_state.n.01 (2)\n",
      "\t president\n",
      "\n",
      "work.n.01 (2)\n",
      "\t job\n",
      "\n",
      "time_unit.n.01 (2)\n",
      "\t day\n",
      "\n",
      "boundary.n.01 (2)\n",
      "\t border\n",
      "\n",
      "edge.n.06 (2)\n",
      "\t border\n",
      "\n",
      "leader.n.01 (2)\n",
      "\t politician\n",
      "\n",
      "ending.n.04 (2)\n",
      "\t victory, triumph\n",
      "\n",
      "success.n.01 (2)\n",
      "\t victory, triumph\n",
      "\n",
      "real_property.n.01 (2)\n",
      "\t land\n",
      "\n",
      "object.n.01 (2)\n",
      "\t land\n",
      "\n",
      "geographical_area.n.01 (1)\n",
      "\t country\n",
      "\n",
      "confederation.n.02 (1)\n",
      "\t nation\n",
      "\n",
      "family.n.04 (1)\n",
      "\t people\n",
      "\n",
      "english.n.01 (1)\n",
      "\t american\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_from_hypernyms(Trump_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2ad0d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_period.n.01 (9)\n",
      "\t year, time, generation, life\n",
      "\n",
      "group.n.01 (5)\n",
      "\t world, people, men\n",
      "\n",
      "people.n.01 (4)\n",
      "\t world, generation, nation, country\n",
      "\n",
      "being.n.01 (3)\n",
      "\t life\n",
      "\n",
      "person.n.01 (3)\n",
      "\t life, child, men\n",
      "\n",
      "male.n.02 (3)\n",
      "\t men\n",
      "\n",
      "direction.n.06 (3)\n",
      "\t rule\n",
      "\n",
      "activity.n.01 (3)\n",
      "\t job, effort, work\n",
      "\n",
      "political_unit.n.01 (2)\n",
      "\t nation, country\n",
      "\n",
      "inhabitant.n.01 (2)\n",
      "\t american\n",
      "\n",
      "experience.n.02 (2)\n",
      "\t world, life\n",
      "\n",
      "freedom.n.01 (2)\n",
      "\t liberty\n",
      "\n",
      "doctrine.n.01 (2)\n",
      "\t creed\n",
      "\n",
      "promise.n.01 (2)\n",
      "\t oath, word\n",
      "\n",
      "generalization.n.02 (2)\n",
      "\t rule, principle\n",
      "\n",
      "law.n.04 (2)\n",
      "\t rule, principle\n",
      "\n",
      "homo.n.02 (2)\n",
      "\t world, men\n",
      "\n",
      "concept.n.01 (2)\n",
      "\t rule\n",
      "\n",
      "duration.n.01 (2)\n",
      "\t value, rule\n",
      "\n",
      "product.n.02 (2)\n",
      "\t job, work\n",
      "\n",
      "work.n.01 (2)\n",
      "\t job\n",
      "\n",
      "head_of_state.n.01 (2)\n",
      "\t president\n",
      "\n",
      "family.n.04 (1)\n",
      "\t people\n",
      "\n",
      "case.n.01 (1)\n",
      "\t time\n",
      "\n",
      "moment.n.01 (1)\n",
      "\t time\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_from_hypernyms(Obama_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d58f3381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "person.n.01 (6)\n",
      "\t slave, case, authority, party\n",
      "\n",
      "time_period.n.01 (6)\n",
      "\t term, time, year\n",
      "\n",
      "concept.n.01 (4)\n",
      "\t section, part, law\n",
      "\n",
      "work.n.01 (4)\n",
      "\t labor, duty, service\n",
      "\n",
      "group.n.01 (3)\n",
      "\t people\n",
      "\n",
      "organization.n.01 (3)\n",
      "\t union, institution, party\n",
      "\n",
      "state.n.02 (3)\n",
      "\t union, office, power\n",
      "\n",
      "social_control.n.01 (3)\n",
      "\t duty, administration, government\n",
      "\n",
      "administrative_district.n.01 (2)\n",
      "\t state\n",
      "\n",
      "attribute.n.02 (2)\n",
      "\t state, time\n",
      "\n",
      "political_unit.n.01 (2)\n",
      "\t union, state\n",
      "\n",
      "executive_department.n.01 (2)\n",
      "\t labor, state\n",
      "\n",
      "beginning.n.05 (2)\n",
      "\t institution, constitution\n",
      "\n",
      "collection.n.01 (2)\n",
      "\t hand, law\n",
      "\n",
      "force.n.04 (2)\n",
      "\t service, law\n",
      "\n",
      "sexual_activity.n.01 (2)\n",
      "\t union, congress\n",
      "\n",
      "happening.n.01 (2)\n",
      "\t union, case\n",
      "\n",
      "container.n.01 (2)\n",
      "\t case\n",
      "\n",
      "activity.n.01 (2)\n",
      "\t service, provision\n",
      "\n",
      "number.n.01 (2)\n",
      "\t majority, minority\n",
      "\n",
      "age.n.03 (2)\n",
      "\t majority, minority\n",
      "\n",
      "administrative_unit.n.01 (2)\n",
      "\t office, authority\n",
      "\n",
      "goal.n.01 (2)\n",
      "\t purpose, object\n",
      "\n",
      "constituent.n.04 (2)\n",
      "\t term, object\n",
      "\n",
      "resoluteness.n.01 (2)\n",
      "\t purpose, decision\n",
      "\n"
     ]
    }
   ],
   "source": [
    "categories_from_hypernyms(Lincoln_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe523ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
